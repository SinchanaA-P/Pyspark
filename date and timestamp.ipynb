{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c375c4-53b4-4dcc-ba81-b3d2db4b517c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+\n|id |dt        |ts                 |\n+---+----------+-------------------+\n|1  |2024-05-10|2024-05-10 18:20:55|\n|2  |2023-12-25|2023-12-25 06:45:10|\n|3  |2022-01-01|2022-01-01 00:00:00|\n|4  |2025-08-30|2025-08-30 23:59:59|\n|5  |2024-02-29|2024-02-29 12:30:45|\n+---+----------+-------------------+\n\n+--------------+\n|current_date()|\n+--------------+\n|    2025-12-12|\n|    2025-12-12|\n|    2025-12-12|\n|    2025-12-12|\n|    2025-12-12|\n+--------------+\n\nroot\n |-- id: string (nullable = true)\n |-- dt: string (nullable = true)\n |-- ts: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#date and timestamp functions\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"date_ts_example\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"2024-05-10\", \"2024-05-10 18:20:55\"),\n",
    "    (\"2\", \"2023-12-25\", \"2023-12-25 06:45:10\"),\n",
    "    (\"3\", \"2022-01-01\", \"2022-01-01 00:00:00\"),\n",
    "    (\"4\", \"2025-08-30\", \"2025-08-30 23:59:59\"),\n",
    "    (\"5\", \"2024-02-29\", \"2024-02-29 12:30:45\")  \n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"dt\", \"ts\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import current_date\n",
    "df.select(current_date()).show()\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4130d68-2254-4c3a-a538-000bbfd296e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|current_date()|\n+--------------+\n|    2025-12-12|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#without df\n",
    "spark.sql(\"SELECT current_date()\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394fd515-a691-4ef7-a20a-49d69d3d95a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n|current_timestamp()       |\n+--------------------------+\n|2025-12-12 06:00:42.826706|\n|2025-12-12 06:00:42.826706|\n|2025-12-12 06:00:42.826706|\n|2025-12-12 06:00:42.826706|\n|2025-12-12 06:00:42.826706|\n+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "df.select(current_timestamp()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89246dae-0496-447e-999d-e4a62a821362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|formatted_date|\n+--------------+\n|    10-05-2024|\n|    25-12-2023|\n|    01-01-2022|\n|    30-08-2025|\n|    29-02-2024|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, struct\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "\n",
    "df.select(date_format(\"dt\", \"dd-MM-yyyy\").alias(\"formatted_date\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae6dfda-5e08-4967-9343-06a14b574f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|to_date(dt, yyyy-MM-dd)|\n+-----------------------+\n|             2024-05-10|\n|             2023-12-25|\n|             2022-01-01|\n|             2025-08-30|\n|             2024-02-29|\n+-----------------------+\n\nroot\n |-- to_date(dt, yyyy-MM-dd): date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#convert string to date\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df.select(to_date(\"dt\", \"yyyy-MM-dd\")).show()\n",
    "df.select(to_date(\"dt\", \"yyyy-MM-dd\")).printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8a0015-4e9e-42bf-bc41-a6d2c0558a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n|to_timestamp(ts, yyyy-MM-dd HH:mm:ss)|\n+-------------------------------------+\n|                  2024-05-10 18:20:55|\n|                  2023-12-25 06:45:10|\n|                  2022-01-01 00:00:00|\n|                  2025-08-30 23:59:59|\n|                  2024-02-29 12:30:45|\n+-------------------------------------+\n\nroot\n |-- to_timestamp(ts, yyyy-MM-dd HH:mm:ss): timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#convert string to timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df.select(to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm:ss\")).show()\n",
    "df.select(to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm:ss\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ed2188-aa45-4b52-ba22-e08cac1d83d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|year(dt)|\n+--------+\n|    2024|\n|    2023|\n|    2022|\n|    2025|\n|    2024|\n+--------+\n\n+---------+\n|month(dt)|\n+---------+\n|        5|\n|       12|\n|        1|\n|        8|\n|        2|\n+---------+\n\n+--------------+\n|dayofmonth(dt)|\n+--------------+\n|            10|\n|            25|\n|             1|\n|            30|\n|            29|\n+--------------+\n\n+--------------+\n|weekofyear(dt)|\n+--------------+\n|            19|\n|            52|\n|            52|\n|            35|\n|             9|\n+--------------+\n\n+--------+\n|hour(ts)|\n+--------+\n|      18|\n|       6|\n|       0|\n|      23|\n|      12|\n+--------+\n\n+----------+\n|minute(ts)|\n+----------+\n|        20|\n|        45|\n|         0|\n|        59|\n|        30|\n+----------+\n\n+----------+\n|second(ts)|\n+----------+\n|        55|\n|        10|\n|         0|\n|        59|\n|        45|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#extraction of date,day and year\n",
    "df.select(year(\"dt\")).show()\n",
    "df.select(month(\"dt\")).show()\n",
    "df.select(dayofmonth(\"dt\")).show()\n",
    "df.select(weekofyear(\"dt\")).show()\n",
    "\n",
    "#extraction of hour,minutes and seconds\n",
    "df.select(hour(\"ts\")).show()\n",
    "df.select(minute(\"ts\")).show()\n",
    "df.select(second(\"ts\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5606328-56a7-439a-a4d7-7249bfcf3900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|date_add(dt, 10)|\n+----------------+\n|      2024-05-20|\n|      2024-01-04|\n|      2022-01-11|\n|      2025-09-09|\n|      2024-03-10|\n+----------------+\n\n+---------------+\n|date_sub(dt, 5)|\n+---------------+\n|     2024-05-05|\n|     2023-12-20|\n|     2021-12-27|\n|     2025-08-25|\n|     2024-02-24|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#add and subtract\n",
    "df.select(date_add(\"dt\", 10)).show()\n",
    "df.select(date_sub(\"dt\", 5)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0898a59d-1946-44a0-800a-7974bdfe5b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n|datediff(2025-12-12, dt)|\n+------------------------+\n|                     581|\n|                     718|\n|                    1441|\n|                     104|\n|                     652|\n+------------------------+\n\n+------------------------+\n|datediff(dt, 2025-12-12)|\n+------------------------+\n|                    -581|\n|                    -718|\n|                   -1441|\n|                    -104|\n|                    -652|\n+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#finding the no of dates between two dates\n",
    "df.select(datediff( lit(\"2025-12-12\"),\"dt\")).show()\n",
    "df.select(datediff(\"dt\", lit(\"2025-12-12\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1191b711-b73d-4105-aef4-7d2fa09810fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n|months_between(dt, 2025-12-12, true)|\n+------------------------------------+\n|                        -19.06451613|\n|                        -23.58064516|\n|                        -47.35483871|\n|                         -3.41935484|\n|                         -21.4516129|\n+------------------------------------+\n\n+------------------------------------+\n|months_between(2025-12-12, dt, true)|\n+------------------------------------+\n|                         19.06451613|\n|                         23.58064516|\n|                         47.35483871|\n|                          3.41935484|\n|                          21.4516129|\n+------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#finding the months between two dates\n",
    "df.select(months_between(\"dt\", lit(\"2025-12-12\"))).show()\n",
    "df.select(months_between( lit(\"2025-12-12\"),\"dt\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "date and timestamp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}